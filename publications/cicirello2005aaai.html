<!DOCTYPE html>
<html lang=en itemscope itemtype="http://schema.org/WebPage" itemref="pageAuthor topNav">
<head>
<meta charset=utf-8>
<link rel="canonical" href="http://www.cicirello.org/publications/cicirello2005aaai.html">
<link itemprop="url" href="http://www.cicirello.org/publications/cicirello2005aaai.html">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="title" content="The Max K-Armed Bandit: A New Model of Exploration Applied to Search Heuristic Selection - AAAI - 2005">
<meta itemprop="name" content="The Max K-Armed Bandit: A New Model of Exploration Applied to Search Heuristic Selection - AAAI - 2005">
<meta name="description" content="The multiarmed bandit is often used as an analogy for the tradeoff between
exploration and exploitation in search problems. The classic problem involves
allocating trials to the arms of a multiarmed slot machine to maximize the
expected sum of rewards. We pose a new variation of the multiarmed bandit, the
Max K-Armed Bandit, in which trials must be allocated among the arms to
maximize the expected best single sample reward of the series of trials.
Motivation for the Max K-Armed Bandit is the allocation of restarts among a set
of multistart stochastic search algorithms. We present an analysis of this Max
K-Armed Bandit showing under certain assumptions that the optimal strategy
allocates trials to the observed best arm at a rate increasing double
exponentially relative to the other arms. This motivates an exploration
strategy that follows a Boltzmann distribution with an exponentially decaying
temperature parameter. We compare this exploration policy to policies that
allocate trials to the observed best arm at rates faster (and slower) than
double exponentially. The results confirm, for two scheduling domains, that the
double exponential increase in the rate of allocations to the observed best
heuristic outperforms the other approaches.">
<meta itemprop="description" content="The multiarmed bandit is often used as an analogy for the tradeoff between
exploration and exploitation in search problems. The classic problem involves
allocating trials to the arms of a multiarmed slot machine to maximize the
expected sum of rewards. We pose a new variation of the multiarmed bandit, the
Max K-Armed Bandit, in which trials must be allocated among the arms to
maximize the expected best single sample reward of the series of trials.
Motivation for the Max K-Armed Bandit is the allocation of restarts among a set
of multistart stochastic search algorithms. We present an analysis of this Max
K-Armed Bandit showing under certain assumptions that the optimal strategy
allocates trials to the observed best arm at a rate increasing double
exponentially relative to the other arms. This motivates an exploration
strategy that follows a Boltzmann distribution with an exponentially decaying
temperature parameter. We compare this exploration policy to policies that
allocate trials to the observed best arm at rates faster (and slower) than
double exponentially. The results confirm, for two scheduling domains, that the
double exponential increase in the rate of allocations to the observed best
heuristic outperforms the other approaches.">
<meta name="citation_title" content="The Max K-Armed Bandit: A New Model of Exploration Applied to Search Heuristic Selection">
<meta name="citation_author" content="Vincent A. Cicirello">
<meta name="citation_author" content="Stephen F. Smith">
<meta name="citation_publication_date" content="2005">
<meta name="citation_date" content="2005">
<meta name="citation_conference_title" content="The Proceedings of the Twentieth National Conference on Artificial Intelligence">
<meta name="citation_volume" content="3">
<meta name="citation_firstpage" content="1355">
<meta name="citation_lastpage" content="1361">
<meta name="citation_pdf_url" content="http://www.cicirello.org/publications/AAAI2005.pdf">
<meta name="citation_abstract_html_url" content="http://www.cicirello.org/publications/cicirello2005aaai.html">
<title>The Max K-Armed Bandit: A New Model of Exploration Applied to Search Heuristic Selection - AAAI - 2005</title>
<style>
header, nav, footer, article, aside, section { display: block; }
footer { clear: both; }
body { background-color: #f6f0bb; font-family: Arial, Helvetica, sans-serif; }
#siteheader, aside, footer { background-color: #bfd9bf; }
#siteheader, aside, footer, article { border: 2px solid #4CAF50; border-radius: 25px; margin-bottom: 5px; padding: 8px; }
h1, h2, h3, h4, h5, h6, strong { color: #862d2d; }
#siteheader h2 { font-size: 2em; }
a { color: #4CAF50; }
a:visited { color:#862d2d; }
nav ul { list-style-type: none; margin: 0px; padding: 0; overflow: hidden; }
nav li { float: left; margin-right: 8px; }
nav li a { display: block; background-color: #f6f0bb; border: 2px solid #4CAF50; text-decoration: none; text-align: center; color: #666; }
nav li:not(:first-child) a { border-bottom: none; border-radius: 12px 12px 0px 0px; padding: 6px 12px; }
#menu-icon { display: none; }
nav li a:visited { color: #666; }
nav li a:hover:not(.active) { background-color: #305030; color: white; }
nav li a.active { background-color: #4CAF50; }
aside { float: left; width: 235px; }
aside h4 { display: inline; }
article { float: right; width: calc(100% - 280px); }
article li { margin-bottom: 8px; margin-top: 8px; }
.publist li { margin-bottom: 16px; }
.publist ul { margin-bottom: 32px; }
section { margin-top: 30px; }
#pubyears { list-style-type: none; margin: 0px; padding: 0px; overflow: auto; }
#pubyears li { float: left; margin: 0px 8px 8px 0px; }
#copyright { text-align: center; color: #862d2d; }
@media screen and (max-width:600px){
article, aside { clear:both; width:calc(100% - 20px); }
nav:not(.open) li a:not(.active) { display: none; }
#menu-icon { display: block; padding: 0px 4px 1px 4px; border-radius: 4px; }
}
</style>
</head>
<body>
<header itemprop="hasPart" itemscope itemtype="http://schema.org/WPHeader" id="siteheader">
<div itemscope>
<h2 id="pageAuthor" itemprop="author copyrightHolder" itemscope itemtype="http://schema.org/Person" itemref="vacData"><a itemprop="mainEntityOfPage url" href="/"><span itemprop="name">Vincent Cicirello</span>, <span itemprop="honorificSuffix">Ph.D.</span> - <span itemprop="jobTitle">Professor of Computer Science</span></a></h2>
<nav itemprop="hasPart" itemscope itemtype="http://schema.org/SiteNavigationElement" id="topNav">
<ul>
<li><a href="javascript:void(0);" onclick="toggleMenu()" id="menu-icon">&#9776;</a></li>
<li><a href="/">Home</a></li>
<li><a href="/publications/">Publications</a></li>
<li><a href="/research/">Research</a></li>
<li><a href="/datasets/">Datasets</a></li>
<li><a href="/software/">Open Source Software</a></li>
<li><a href="/teaching/">Teaching</a></li>
<li><a href="/search/">Site Search</a></li>
</ul>
</nav>
</div>
</header>
<article itemprop="mainEntity" itemscope itemtype="http://schema.org/ScholarlyArticle">
<header>
<h2><a href="/publications/AAAI2005.pdf"><span itemprop="name">The Max K-Armed Bandit: A New Model of Exploration Applied to Search Heuristic Selection</span></a></h2>
<h3><span itemprop="author" itemscope itemtype="http://schema.org/Person"><a itemprop="url mainEntityOfPage" href="/"><span itemprop="name">Vincent A. Cicirello</span></a></span> and <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span itemprop="name">Stephen F. Smith</span></span></h3>
<h4><span itemprop="isPartOf" itemscope itemtype="http://schema.org/Book">In <i><span itemprop="name">The Proceedings of the Twentieth National Conference on Artificial Intelligence</span></i>,
<span itemprop="hasPart" itemscope itemtype="http://schema.org/PublicationVolume">volume
<span itemprop="volumeNumber">3</span></span></span>, pages <span itemprop="pageStart">1355</span>-<span itemprop="pageEnd">1361</span>. <span itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><span itemprop="name">AAAI Press</span></span>, <time datetime="2005-07" itemprop="datePublished">July 2005</time>.</h4>
<h4>Winner of the <span itemprop="award">AAAI 2005 Outstanding Paper Award</span>.</h4>
</header>
<p><a itemprop="url" href="/publications/AAAI2005.pdf">[PDF]</a> <a href="/publications/cicirello2005aaai.bib">[BIB]</a> <a itemprop="sameAs" href="http://www.aaai.org/Library/AAAI/2005/aaai05-215.php">[PUB]</a></p>
<h4>Abstract</h4>
<div itemprop="description"><p>The multiarmed bandit is often used as an analogy for the tradeoff between
exploration and exploitation in search problems. The classic problem involves
allocating trials to the arms of a multiarmed slot machine to maximize the
expected sum of rewards. We pose a new variation of the multiarmed bandit, the
Max K-Armed Bandit, in which trials must be allocated among the arms to
maximize the expected best single sample reward of the series of trials.
Motivation for the Max K-Armed Bandit is the allocation of restarts among a set
of multistart stochastic search algorithms. We present an analysis of this Max
K-Armed Bandit showing under certain assumptions that the optimal strategy
allocates trials to the observed best arm at a rate increasing double
exponentially relative to the other arms. This motivates an exploration
strategy that follows a Boltzmann distribution with an exponentially decaying
temperature parameter. We compare this exploration policy to policies that
allocate trials to the observed best arm at rates faster (and slower) than
double exponentially. The results confirm, for two scheduling domains, that the
double exponential increase in the rate of allocations to the observed best
heuristic outperforms the other approaches.</p></div>
</article>
<aside itemprop="hasPart" itemscope itemtype="http://schema.org/WPSideBar">
<div itemscope><div id="vacData">
<h3><span itemprop="alternateName">Vincent A. Cicirello</span>, Ph.D.</h3>
<h4>Current Position:</h4><br>
Professor of Computer Science and Information Systems<br><br>
<h4>Mailing Address:</h4><br>
School of Business<br>
Stockton University<br>
101 Vera King Farris Dr<br>
Galloway, NJ 08205<br><br>
<h4>E-mail:</h4>
<iframe src="/images/addr.html" width="177" height="19" style="display:inline;border-style:none;border:0px;margin:0px;padding:0px;">Contact Address</iframe>
<br><br>
<h4>Phone:</h4>
(609) 626-3526<br><br>
<h4>Social Media:</h4><br>
<a itemprop="sameAs" href="http://www.linkedin.com/in/vacicirello"><img src="https://static.licdn.com/scds/common/u/img/webpromo/btn_liprofile_blue_80x15.png" width="80" height="15" alt="View Vincent A. Cicirello's profile on LinkedIn"></a>
<a itemprop="sameAs" href="https://www.researchgate.net/profile/Vincent_Cicirello"><img alt="Follow me on ResearchGate" src="https://www.researchgate.net/images/public/profile_share_badge.png" width="100"></a>
</div></div>
</aside>
<footer itemprop="hasPart" itemscope itemtype="http://schema.org/WPFooter">
<div>
<a href="http://jigsaw.w3.org/css-validator/check/referer">
<img style="border:0;width:88px;height:31px" src="http://jigsaw.w3.org/css-validator/images/vcss" alt="Valid CSS!">
</a>
</div>
<div id="copyright">Copyright &copy; 1996-<span id="year"></span> Vincent A. Cicirello.</div>
<script>document.getElementById("year").innerHTML = new Date().getFullYear();</script>
</footer>
<script>function toggleMenu() { var x = document.getElementById("topNav"); x.className = (x.className === "") ? "open" : x.className = ""; }</script>
</body>
</html>
